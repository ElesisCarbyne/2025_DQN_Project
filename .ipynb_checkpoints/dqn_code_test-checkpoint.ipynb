{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14993,
     "status": "ok",
     "timestamp": 1754126112193,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "--Nh4Yt9_fh1",
    "outputId": "2cac765c-a500-4b22-bc06-ef2ba2e76e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-summary in c:\\users\\kranv\\miniconda3\\envs\\gradenv\\lib\\site-packages (1.4.5)\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10820,
     "status": "ok",
     "timestamp": 1754070469041,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "Kbjld-fW--fL",
    "outputId": "32e28277-90fb-4431-f2b6-78090f9bb6ec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "├─Sequential: 1-1                                  [-1, 576, 1, 1]           --\n",
      "|    └─Conv2dNormActivation: 2-1                   [-1, 16, 14, 14]          --\n",
      "|    |    └─Conv2d: 3-1                            [-1, 16, 14, 14]          432\n",
      "|    |    └─BatchNorm2d: 3-2                       [-1, 16, 14, 14]          32\n",
      "|    |    └─Hardswish: 3-3                         [-1, 16, 14, 14]          --\n",
      "|    └─InvertedResidual: 2-2                       [-1, 16, 7, 7]            --\n",
      "|    |    └─Sequential: 3-4                        [-1, 16, 7, 7]            744\n",
      "|    └─InvertedResidual: 2-3                       [-1, 24, 4, 4]            --\n",
      "|    |    └─Sequential: 3-5                        [-1, 24, 4, 4]            3,864\n",
      "|    └─InvertedResidual: 2-4                       [-1, 24, 4, 4]            --\n",
      "|    |    └─Sequential: 3-6                        [-1, 24, 4, 4]            5,416\n",
      "|    └─InvertedResidual: 2-5                       [-1, 40, 2, 2]            --\n",
      "|    |    └─Sequential: 3-7                        [-1, 40, 2, 2]            13,736\n",
      "|    └─InvertedResidual: 2-6                       [-1, 40, 2, 2]            --\n",
      "|    |    └─Sequential: 3-8                        [-1, 40, 2, 2]            57,264\n",
      "|    └─InvertedResidual: 2-7                       [-1, 40, 2, 2]            --\n",
      "|    |    └─Sequential: 3-9                        [-1, 40, 2, 2]            57,264\n",
      "|    └─InvertedResidual: 2-8                       [-1, 48, 2, 2]            --\n",
      "|    |    └─Sequential: 3-10                       [-1, 48, 2, 2]            21,968\n",
      "|    └─InvertedResidual: 2-9                       [-1, 48, 2, 2]            --\n",
      "|    |    └─Sequential: 3-11                       [-1, 48, 2, 2]            29,800\n",
      "|    └─InvertedResidual: 2-10                      [-1, 96, 1, 1]            --\n",
      "|    |    └─Sequential: 3-12                       [-1, 96, 1, 1]            91,848\n",
      "|    └─InvertedResidual: 2-11                      [-1, 96, 1, 1]            --\n",
      "|    |    └─Sequential: 3-13                       [-1, 96, 1, 1]            294,096\n",
      "|    └─InvertedResidual: 2-12                      [-1, 96, 1, 1]            --\n",
      "|    |    └─Sequential: 3-14                       [-1, 96, 1, 1]            294,096\n",
      "|    └─Conv2dNormActivation: 2-13                  [-1, 576, 1, 1]           --\n",
      "|    |    └─Conv2d: 3-15                           [-1, 576, 1, 1]           55,296\n",
      "|    |    └─BatchNorm2d: 3-16                      [-1, 576, 1, 1]           1,152\n",
      "|    |    └─Hardswish: 3-17                        [-1, 576, 1, 1]           --\n",
      "├─AdaptiveAvgPool2d: 1-2                           [-1, 576, 1, 1]           --\n",
      "├─Sequential: 1-3                                  [-1, 1000]                --\n",
      "|    └─Linear: 2-14                                [-1, 1024]                590,848\n",
      "|    └─Hardswish: 2-15                             [-1, 1024]                --\n",
      "|    └─Dropout: 2-16                               [-1, 1024]                --\n",
      "|    └─Linear: 2-17                                [-1, 1000]                1,025,000\n",
      "====================================================================================================\n",
      "Total params: 2,542,856\n",
      "Trainable params: 2,542,856\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 6.93\n",
      "====================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 9.70\n",
      "Estimated Total Size (MB): 9.78\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "├─Sequential: 1-1                                  [-1, 576, 1, 1]           --\n",
       "|    └─Conv2dNormActivation: 2-1                   [-1, 16, 14, 14]          --\n",
       "|    |    └─Conv2d: 3-1                            [-1, 16, 14, 14]          432\n",
       "|    |    └─BatchNorm2d: 3-2                       [-1, 16, 14, 14]          32\n",
       "|    |    └─Hardswish: 3-3                         [-1, 16, 14, 14]          --\n",
       "|    └─InvertedResidual: 2-2                       [-1, 16, 7, 7]            --\n",
       "|    |    └─Sequential: 3-4                        [-1, 16, 7, 7]            744\n",
       "|    └─InvertedResidual: 2-3                       [-1, 24, 4, 4]            --\n",
       "|    |    └─Sequential: 3-5                        [-1, 24, 4, 4]            3,864\n",
       "|    └─InvertedResidual: 2-4                       [-1, 24, 4, 4]            --\n",
       "|    |    └─Sequential: 3-6                        [-1, 24, 4, 4]            5,416\n",
       "|    └─InvertedResidual: 2-5                       [-1, 40, 2, 2]            --\n",
       "|    |    └─Sequential: 3-7                        [-1, 40, 2, 2]            13,736\n",
       "|    └─InvertedResidual: 2-6                       [-1, 40, 2, 2]            --\n",
       "|    |    └─Sequential: 3-8                        [-1, 40, 2, 2]            57,264\n",
       "|    └─InvertedResidual: 2-7                       [-1, 40, 2, 2]            --\n",
       "|    |    └─Sequential: 3-9                        [-1, 40, 2, 2]            57,264\n",
       "|    └─InvertedResidual: 2-8                       [-1, 48, 2, 2]            --\n",
       "|    |    └─Sequential: 3-10                       [-1, 48, 2, 2]            21,968\n",
       "|    └─InvertedResidual: 2-9                       [-1, 48, 2, 2]            --\n",
       "|    |    └─Sequential: 3-11                       [-1, 48, 2, 2]            29,800\n",
       "|    └─InvertedResidual: 2-10                      [-1, 96, 1, 1]            --\n",
       "|    |    └─Sequential: 3-12                       [-1, 96, 1, 1]            91,848\n",
       "|    └─InvertedResidual: 2-11                      [-1, 96, 1, 1]            --\n",
       "|    |    └─Sequential: 3-13                       [-1, 96, 1, 1]            294,096\n",
       "|    └─InvertedResidual: 2-12                      [-1, 96, 1, 1]            --\n",
       "|    |    └─Sequential: 3-14                       [-1, 96, 1, 1]            294,096\n",
       "|    └─Conv2dNormActivation: 2-13                  [-1, 576, 1, 1]           --\n",
       "|    |    └─Conv2d: 3-15                           [-1, 576, 1, 1]           55,296\n",
       "|    |    └─BatchNorm2d: 3-16                      [-1, 576, 1, 1]           1,152\n",
       "|    |    └─Hardswish: 3-17                        [-1, 576, 1, 1]           --\n",
       "├─AdaptiveAvgPool2d: 1-2                           [-1, 576, 1, 1]           --\n",
       "├─Sequential: 1-3                                  [-1, 1000]                --\n",
       "|    └─Linear: 2-14                                [-1, 1024]                590,848\n",
       "|    └─Hardswish: 2-15                             [-1, 1024]                --\n",
       "|    └─Dropout: 2-16                               [-1, 1024]                --\n",
       "|    └─Linear: 2-17                                [-1, 1000]                1,025,000\n",
       "====================================================================================================\n",
       "Total params: 2,542,856\n",
       "Trainable params: 2,542,856\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 6.93\n",
       "====================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.07\n",
       "Params size (MB): 9.70\n",
       "Estimated Total Size (MB): 9.78\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models.mobilenetv3 import MobileNet_V3_Small_Weights\n",
    "from torchsummary import summary\n",
    "\n",
    "model = models.mobilenet_v3_small()\n",
    "summary(model, (3, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1751055200515,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "djXwCdDhAecX",
    "outputId": "a2126ad3-449b-4950-d6cd-cccdfe7da74d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the whole structure of model excluding classifier:\n",
      "Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "        (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "        (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "        (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): Conv2dNormActivation(\n",
      "    (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      ")\n",
      "\n",
      "model stack 1:\n",
      "Conv2dNormActivation(\n",
      "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "\n",
      "type of model stack 1:\n",
      "<class 'torchvision.ops.misc.Conv2dNormActivation'>\n",
      "\n",
      "layer1 of model stack 1:\n",
      "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\n",
      "type of layer1 of model stack 1:\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "\n",
      "stride of layer1 of model stack 1:\n",
      "(1, 1)\n",
      "\n",
      "model's classifier:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n",
      "\n",
      "last layer of model's classifier:\n",
      "Linear(in_features=1024, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# type(model.features)\n",
    "print(f\"the whole structure of model excluding classifier:\\n{model.features}\\n\")\n",
    "print(f\"model stack 1:\\n{model.features[0]}\\n\")\n",
    "print(f\"type of model stack 1:\\n{type(model.features[0])}\\n\")\n",
    "print(f\"layer1 of model stack 1:\\n{model.features[0][0]}\\n\")\n",
    "print(f\"type of layer1 of model stack 1:\\n{type(model.features[0][0])}\\n\")\n",
    "print(f\"stride of layer1 of model stack 1:\\n{model.features[0][0].stride}\\n\")\n",
    "print(f\"model's classifier:\\n{model.classifier}\\n\")\n",
    "print(f\"last layer of model's classifier:\\n{model.classifier[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1751054167536,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "3EmTek7e9CEi",
    "outputId": "97c5bb59-d00a-4eff-9b56-10f3cc5ee1f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Conv2d', True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(model.features[0][0]) == \"torch.nn.modules.conv.Conv2d\"\n",
    "# dir(model.features[0][0])\n",
    "# model.features[0][0].__class__ == \"torch.nn.modules.conv.Conv2d\"\n",
    "model.features[0][0]._get_name(), model.features[0][0]._get_name() == \"Conv2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1751054399336,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "nlew1aj097zM",
    "outputId": "908a5f7f-bb6d-4778-abf5-d9c68d345ac4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(model.features)\n",
    "# model.features.__sizeof__()\n",
    "model.features.__len__(), len(model.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1751055025757,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "7ugOso08-py-",
    "outputId": "3639b06c-20e7-49a2-fddf-3fd0459fd6c8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2dNormActivation(\n",
      "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "      (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "      (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "      (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "      (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "      (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Conv2dNormActivation(\n",
      "  (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d\n",
      "BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "BatchNorm2d\n",
      "Hardswish()\n",
      "Hardswish\n"
     ]
    }
   ],
   "source": [
    "for stack in model.features:\n",
    "  print(stack)\n",
    "\n",
    "for stack in model.features:\n",
    "  for layer in stack:\n",
    "    print(layer)\n",
    "    print(layer._get_name())\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1751297167114,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "8EKdIPq4DbMh",
    "outputId": "7d993032-b5e4-408f-c18c-b174bc822e39",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: features.0.0, Layer: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Layer Name: features.1.block.0.0, Layer: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "Layer Name: features.1.block.1.fc1, Layer: Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.1.block.1.fc2, Layer: Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.1.block.2.0, Layer: Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.2.block.0.0, Layer: Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.2.block.1.0, Layer: Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "Layer Name: features.2.block.2.0, Layer: Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.3.block.0.0, Layer: Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.3.block.1.0, Layer: Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "Layer Name: features.3.block.2.0, Layer: Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.4.block.0.0, Layer: Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.4.block.1.0, Layer: Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "Layer Name: features.4.block.2.fc1, Layer: Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.4.block.2.fc2, Layer: Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.4.block.3.0, Layer: Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.5.block.0.0, Layer: Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.5.block.1.0, Layer: Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "Layer Name: features.5.block.2.fc1, Layer: Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.5.block.2.fc2, Layer: Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.5.block.3.0, Layer: Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.6.block.0.0, Layer: Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.6.block.1.0, Layer: Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "Layer Name: features.6.block.2.fc1, Layer: Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.6.block.2.fc2, Layer: Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.6.block.3.0, Layer: Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.7.block.0.0, Layer: Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.7.block.1.0, Layer: Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "Layer Name: features.7.block.2.fc1, Layer: Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.7.block.2.fc2, Layer: Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.7.block.3.0, Layer: Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.8.block.0.0, Layer: Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.8.block.1.0, Layer: Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "Layer Name: features.8.block.2.fc1, Layer: Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.8.block.2.fc2, Layer: Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.8.block.3.0, Layer: Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.9.block.0.0, Layer: Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.9.block.1.0, Layer: Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
      "Layer Name: features.9.block.2.fc1, Layer: Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.9.block.2.fc2, Layer: Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.9.block.3.0, Layer: Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.10.block.0.0, Layer: Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.10.block.1.0, Layer: Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "Layer Name: features.10.block.2.fc1, Layer: Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.10.block.2.fc2, Layer: Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.10.block.3.0, Layer: Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.11.block.0.0, Layer: Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.11.block.1.0, Layer: Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "Layer Name: features.11.block.2.fc1, Layer: Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.11.block.2.fc2, Layer: Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "Layer Name: features.11.block.3.0, Layer: Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Layer Name: features.12.0, Layer: Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "# list(model.named_modules())\n",
    "# for name, module in model.named_modules():\n",
    "#   print(\"------------------------------------\")\n",
    "#   print(name, module)\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        module.stride = (1, 1)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        print(f\"Layer Name: {name}, Layer: {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1751055050066,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "BKQqAcVN_ohl",
    "outputId": "4cef245d-d58f-47a4-d586-1166cd3d6c96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('InvertedResidual',\n",
       " {'block': Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): Conv2dNormActivation(\n",
       "      (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (2): SqueezeExcitation(\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (activation): ReLU()\n",
       "      (scale_activation): Hardsigmoid()\n",
       "    )\n",
       "    (3): Conv2dNormActivation(\n",
       "      (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )},\n",
       " Sequential(\n",
       "   (0): Conv2dNormActivation(\n",
       "     (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "     (2): Hardswish()\n",
       "   )\n",
       "   (1): Conv2dNormActivation(\n",
       "     (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "     (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "     (2): Hardswish()\n",
       "   )\n",
       "   (2): SqueezeExcitation(\n",
       "     (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "     (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "     (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "     (activation): ReLU()\n",
       "     (scale_activation): Hardsigmoid()\n",
       "   )\n",
       "   (3): Conv2dNormActivation(\n",
       "     (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Conv2dNormActivation(\n",
       "   (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "   (2): Hardswish()\n",
       " ))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.features[-2], type(model.features[-2]), model.features[-2]._get_name()\n",
    "# dir(model.features[-2])\n",
    "model.features[-2]._get_name(), model.features[-2]._modules, model.features[-2]._modules[\"block\"], model.features[-2]._modules[\"block\"][0]\n",
    "\n",
    "# for stack in model.features:\n",
    "#   if stack._get_name() == \"Conv2dNormActivation\": # stack 이름이 \"Conv2dNormActivation\"인 경우\n",
    "#     for layer in stack:\n",
    "#       if layer._get_name() == \"Conv2d\":\n",
    "#         layer.stride = (1, 1)\n",
    "#   else: # stack 이름이 \"InvertedResidual\"인 경우\n",
    "#     for sub_stack in stack._modules[\"block\"]:\n",
    "#       for sub_layer in sub_stack:\n",
    "#         if sub_layer._get_name() == \"Conv2d\":\n",
    "#           sub_layer.stride = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1753471319607,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "4GmLq-Sldlh5",
    "outputId": "2adaca45-17ea-4b2b-d088-8c8922406b4e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7000\n",
      "0.6940\n",
      "0.6881\n",
      "0.6823\n",
      "0.6765\n",
      "0.6707\n",
      "0.6651\n",
      "0.6594\n",
      "0.6539\n",
      "0.6484\n",
      "0.6429\n",
      "0.6375\n",
      "0.6322\n",
      "0.6269\n",
      "0.6216\n",
      "0.6164\n",
      "0.6113\n",
      "0.6062\n",
      "0.6012\n",
      "0.5962\n",
      "0.5912\n",
      "0.5864\n",
      "0.5815\n",
      "0.5767\n",
      "0.5720\n",
      "0.5673\n",
      "0.5626\n",
      "0.5580\n",
      "0.5535\n",
      "0.5490\n",
      "0.5445\n",
      "0.5401\n",
      "0.5357\n",
      "0.5314\n",
      "0.5271\n",
      "0.5228\n",
      "0.5186\n",
      "0.5144\n",
      "0.5103\n",
      "0.5062\n",
      "0.5022\n",
      "0.4982\n",
      "0.4942\n",
      "0.4903\n",
      "0.4864\n",
      "0.4826\n",
      "0.4788\n",
      "0.4750\n",
      "0.4713\n",
      "0.4676\n",
      "0.4639\n",
      "0.4603\n",
      "0.4567\n",
      "0.4532\n",
      "0.4496\n",
      "0.4462\n",
      "0.4427\n",
      "0.4393\n",
      "0.4359\n",
      "0.4326\n",
      "0.4293\n",
      "0.4260\n",
      "0.4228\n",
      "0.4196\n",
      "0.4164\n",
      "0.4132\n",
      "0.4101\n",
      "0.4070\n",
      "0.4040\n",
      "0.4009\n",
      "0.3980\n",
      "0.3950\n",
      "0.3921\n",
      "0.3891\n",
      "0.3863\n",
      "0.3834\n",
      "0.3806\n",
      "0.3778\n",
      "0.3750\n",
      "0.3723\n",
      "0.3696\n",
      "0.3669\n",
      "0.3643\n",
      "0.3616\n",
      "0.3590\n",
      "0.3564\n",
      "0.3539\n",
      "0.3514\n",
      "0.3489\n",
      "0.3464\n",
      "0.3439\n",
      "0.3415\n",
      "0.3391\n",
      "0.3367\n",
      "0.3344\n",
      "0.3320\n",
      "0.3297\n",
      "0.3274\n",
      "0.3252\n",
      "0.3229\n",
      "0.3207\n",
      "0.3185\n",
      "0.3164\n",
      "0.3142\n",
      "0.3121\n",
      "0.3100\n",
      "0.3079\n",
      "0.3058\n",
      "0.3038\n",
      "0.3017\n",
      "0.2997\n",
      "0.2977\n",
      "0.2958\n",
      "0.2938\n",
      "0.2919\n",
      "0.2900\n",
      "0.2881\n",
      "0.2862\n",
      "0.2844\n",
      "0.2825\n",
      "0.2807\n",
      "0.2789\n",
      "0.2771\n",
      "0.2754\n",
      "0.2736\n",
      "0.2719\n",
      "0.2702\n",
      "0.2685\n",
      "0.2668\n",
      "0.2652\n",
      "0.2635\n",
      "0.2619\n",
      "0.2603\n",
      "0.2587\n",
      "0.2571\n",
      "0.2555\n",
      "0.2540\n",
      "0.2525\n",
      "0.2509\n",
      "0.2494\n",
      "0.2480\n",
      "0.2465\n",
      "0.2450\n",
      "0.2436\n",
      "0.2422\n",
      "0.2407\n",
      "0.2393\n",
      "0.2380\n",
      "0.2366\n",
      "0.2352\n",
      "0.2339\n",
      "0.2325\n",
      "0.2312\n",
      "0.2299\n",
      "0.2286\n",
      "0.2273\n",
      "0.2261\n",
      "0.2248\n",
      "0.2236\n",
      "0.2224\n",
      "0.2211\n",
      "0.2199\n",
      "0.2187\n",
      "0.2176\n",
      "0.2164\n",
      "0.2152\n",
      "0.2141\n",
      "0.2129\n",
      "0.2118\n",
      "0.2107\n",
      "0.2096\n",
      "0.2085\n",
      "0.2074\n",
      "0.2064\n",
      "0.2053\n",
      "0.2043\n",
      "0.2032\n",
      "0.2022\n",
      "0.2012\n",
      "0.2002\n",
      "0.1992\n",
      "0.1982\n",
      "0.1972\n",
      "0.1962\n",
      "0.1953\n",
      "0.1943\n",
      "0.1934\n",
      "0.1925\n",
      "0.1916\n",
      "0.1906\n",
      "0.1897\n",
      "0.1888\n",
      "0.1880\n",
      "0.1871\n",
      "0.1862\n",
      "0.1854\n",
      "0.1845\n",
      "0.1837\n",
      "0.1828\n",
      "0.1820\n",
      "0.1812\n",
      "0.1804\n",
      "0.1796\n",
      "0.1788\n",
      "0.1780\n",
      "0.1772\n",
      "0.1765\n",
      "0.1757\n",
      "0.1750\n",
      "0.1742\n",
      "0.1735\n",
      "0.1727\n",
      "0.1720\n",
      "0.1713\n",
      "0.1706\n",
      "0.1699\n",
      "0.1692\n",
      "0.1685\n",
      "0.1678\n",
      "0.1672\n",
      "0.1665\n",
      "0.1658\n",
      "0.1652\n",
      "0.1645\n",
      "0.1639\n",
      "0.1632\n",
      "0.1626\n",
      "0.1620\n",
      "0.1614\n",
      "0.1608\n",
      "0.1602\n",
      "0.1596\n",
      "0.1590\n",
      "0.1584\n",
      "0.1578\n",
      "0.1572\n",
      "0.1567\n",
      "0.1561\n",
      "0.1555\n",
      "0.1550\n",
      "0.1544\n",
      "0.1539\n",
      "0.1534\n",
      "0.1528\n",
      "0.1523\n",
      "0.1518\n",
      "0.1513\n",
      "0.1508\n",
      "0.1502\n",
      "0.1497\n",
      "0.1493\n",
      "0.1488\n",
      "0.1483\n",
      "0.1478\n",
      "0.1473\n",
      "0.1468\n",
      "0.1464\n",
      "0.1459\n",
      "0.1455\n",
      "0.1450\n",
      "0.1446\n",
      "0.1441\n",
      "0.1437\n",
      "0.1432\n",
      "0.1428\n",
      "0.1424\n",
      "0.1420\n",
      "0.1416\n",
      "0.1411\n",
      "0.1407\n",
      "0.1403\n",
      "0.1399\n",
      "0.1395\n",
      "0.1391\n",
      "0.1387\n",
      "0.1384\n",
      "0.1380\n",
      "0.1376\n",
      "0.1372\n",
      "0.1369\n",
      "0.1365\n",
      "0.1361\n",
      "0.1358\n",
      "0.1354\n",
      "0.1351\n",
      "0.1347\n",
      "0.1344\n",
      "0.1340\n",
      "0.1337\n",
      "0.1333\n",
      "0.1330\n",
      "0.1327\n",
      "0.1324\n",
      "0.1320\n",
      "0.1317\n",
      "0.1314\n",
      "0.1311\n",
      "0.1308\n",
      "0.1305\n",
      "0.1302\n"
     ]
    }
   ],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def eps_decay(steps_done):\n",
    "  eps_start = 0.7\n",
    "  # eps_start = 0.4\n",
    "  eps_end = 0.1\n",
    "  # eps_end = 0.05\n",
    "  eps_decay = 100\n",
    "\n",
    "  eps_threshold = eps_end + (eps_start - eps_end) * np.exp(-1. * steps_done / eps_decay)\n",
    "\n",
    "  return eps_threshold\n",
    "\n",
    "for _ in range(300):\n",
    "  print(f\"{eps_decay(steps_done):.4f}\")\n",
    "  steps_done += 1\n",
    "\n",
    "# (20, 6), (4, 10), 16 + 4 = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1751306791816,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "3wkcLmMbAYLC",
    "outputId": "9fc39c77-3e21-448b-aab9-25602ca06c72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.999000499833375\n",
      "0.9980019986673331\n",
      "0.997004495503373\n",
      "0.9960079893439915\n",
      "0.9950124791926823\n",
      "0.9940179640539353\n",
      "0.9930244429332351\n",
      "0.9920319148370607\n",
      "0.9910403787728836\n",
      "0.9900498337491681\n",
      "0.9890602787753687\n",
      "0.9880717128619305\n",
      "0.9870841350202876\n",
      "0.9860975442628619\n",
      "0.9851119396030626\n",
      "0.9841273200552851\n",
      "0.9831436846349096\n",
      "0.9821610323583008\n",
      "0.981179362242806\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "  print(np.exp(-1. * i / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1751653991744,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "j9Vn38nBgWg3",
    "outputId": "78642de8-3c39-4dd2-90da-85e908813271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1000]),\n",
       " tensor([834], device='cuda:0'),\n",
       " torch.int64,\n",
       " torch.Size([1]),\n",
       " torch.Size([1, 1]),\n",
       " torch.int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = torch.randn((1, 3, 28, 28), device=\"cuda\")\n",
    "# print(temp_data.shape)\n",
    "model.eval()\n",
    "result = model(temp_data).to(\"cuda\")\n",
    "print(type(result), result.dtype)\n",
    "result.shape, result.argmax(dim=1), result.argmax(dim=1).dtype, result.argmax(dim=1).shape, result.argmax(dim=1).reshape(1,1).shape, \\\n",
    "torch.long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1751660216210,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "z7XyV-BuvLU5",
    "outputId": "c217f265-98a4-4682-81e1-9c7d3f266dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[342]], device='cuda:0') tensor([[342]], device='cuda:0')\n",
      "torch.Size([2, 3, 28, 28])\n",
      "10\n",
      "<class 'list'>\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([[1]]),\n",
       "  tensor([[2]]),\n",
       "  tensor([[1]]),\n",
       "  tensor([[0]]),\n",
       "  tensor([[1]]),\n",
       "  tensor([[1]]),\n",
       "  tensor([[1]]),\n",
       "  tensor([[0]]),\n",
       "  tensor([[3]]),\n",
       "  tensor([[3]])],\n",
       " tensor([[1],\n",
       "         [2],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [3],\n",
       "         [3]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_state = torch.randn((1, 3, 28, 28), device=\"cuda\")\n",
    "model.eval()\n",
    "# result = model(temp_data).to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "  q_values = model(cur_state).to(\"cuda\")\n",
    "  q_values2 = model(cur_state).to(\"cuda\").argmax(dim=1).reshape(1, 1)\n",
    "print(q_values.argmax(dim=1).reshape(1, 1), q_values2)\n",
    "\n",
    "a = torch.randn(3, 28, 28)\n",
    "b = torch.randn(3, 28, 28)\n",
    "print(torch.stack([a, b], dim=0).shape)\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "buffer = deque([], maxlen=50)\n",
    "for i in range(10):\n",
    " buffer.append((torch.randn(3, 28, 28), torch.randint(0, 4, size=(1, 1))))\n",
    "print(len(buffer))\n",
    "temp = random.sample(buffer, 5)\n",
    "print(type(temp))\n",
    "print(torch.concat([item[1] for item in buffer], dim=0) + torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))\n",
    "[item[1] for item in buffer], torch.concat([item[1] for item in buffer], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1751910763961,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "q8-fQSekAms2",
    "outputId": "02d49e78-ddd2-4bae-fc3f-f63a2433be19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3]),\n",
       " 3,\n",
       " torch.Size([1]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " torch.Size([10, 1]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3,4]]).argmax(dim=1), torch.tensor([[1,2,3,4]]).argmax(dim=1).item(), torch.tensor([[1,2,3,4]]).argmax(dim=1).shape, \\\n",
    "torch.ones((10, )), torch.ones((10, )).reshape(10, -1), torch.ones((10, )).reshape(10, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1752263667172,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "6ZmbU3VTAYW9",
    "outputId": "b5257612-4c89-4382-859f-2e17b4f08ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1]) 3 <class 'int'> 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " tensor([[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [7],\n",
       "         [8],\n",
       "         [9]]),\n",
       " tensor([[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [7],\n",
       "         [8],\n",
       "         [9]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "a = torch.tensor([[1,2,3,4]]).argmax(dim=1)\n",
    "print(a.shape, a.item(), type(a.item()), random.randint(0, 3))\n",
    "b = [0,1,2,3,4,5,6,7,8,9]\n",
    "torch.tensor(b), torch.tensor(b).reshape((10, 1)), torch.tensor(b).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1752273227176,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "_qJ6303IgPvw",
    "outputId": "ee5ac998-35c0-4047-8114-8bc2b6d39b51"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = [1,2,3,4,5,6,7,8,9,10]\n",
    "b = [6,6,6,6,6,6,6,6,6,6]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(a, alpha=0.6, linewidth=2)\n",
    "plt.plot(b, alpha=0.6, linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1752272912487,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "YAVmwCq4k6IA",
    "outputId": "010ae17b-927d-4754-e76d-909397b447c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(2.0),\n",
       " np.float64(3.0),\n",
       " np.float64(4.0),\n",
       " np.float64(5.0),\n",
       " np.float64(6.0),\n",
       " np.float64(7.0),\n",
       " np.float64(8.0),\n",
       " np.float64(9.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "temp = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "vec = np.ones(3)\n",
    "window_size = 3\n",
    "\n",
    "[(temp[i:i+3] @ vec) / window_size for i in range(len(temp) - 3 + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1751911319210,
     "user": {
      "displayName": "WG Oh",
      "userId": "06069895532087570057"
     },
     "user_tz": -540
    },
    "id": "7TCw0YDM_kd2",
    "outputId": "af7b0667-f4e5-4023-ef93-57b6051b2778"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " deque([([1, 2, 3, 4, 5], 0, True, tensor([1, 0])),\n",
       "        ([6, 7, 8, 9, 10], 1, False, tensor([2, 1]))]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "buffer = deque([], maxlen=50)\n",
    "buffer.append(([1,2,3,4,5], 0, True, torch.tensor([1, 0])))\n",
    "buffer.append(([6,7,8,9,10], 1, False, torch.tensor([2, 1])))\n",
    "len(buffer), buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([(1, 2, 3, 4, 5, 6),\n",
       "       (1, 2, 3, 4, 5, 6),\n",
       "       (1, 2, 3, 4, 5, 6),\n",
       "       (1, 2, 3, 4, 5, 6)],\n",
       "      maxlen=100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "buffer = deque([], maxlen=100)\n",
    "temp = (1,2,3,4,5,6)\n",
    "buffer.append(temp)\n",
    "buffer.append(temp)\n",
    "buffer.extend([temp, temp])\n",
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\DQNs\\checkpoints\\checkpoint_1_100_2025-08-03.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('checkpoint_1_100_2025-08-03.pth',\n",
       " ['checkpoint', '1', '100', '2025-08-03.pth'],\n",
       " ['1', '100'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "import os\n",
    "\n",
    "env_creation_cnt = 1\n",
    "ep_cnt_in_env = 100\n",
    "path = os.path.join(os.getcwd(), \"checkpoints\", f\"checkpoint_{env_creation_cnt}_{ep_cnt_in_env}_{str(date.today())}.pth\")\n",
    "print(path)\n",
    "path.split('\\\\')[-1], path.split('\\\\')[-1].split('_'), path.split('\\\\')[-1].split('_')[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64])\n",
      "<class 'torch.nn.modules.flatten.Flatten'>\n",
      "torch.Size([1, 64]) tensor([[-0.1901, -0.3799,  0.8778,  1.6884, -0.9319,  0.8947,  1.3217,  0.5121,\n",
      "          0.9027,  2.1910, -1.5464, -0.3817, -0.0269,  0.5727,  0.3448,  0.2080,\n",
      "         -1.4270, -0.1481, -0.8755, -1.1194, -0.0504,  0.5846, -0.2131,  0.0817,\n",
      "         -0.1633,  0.6947,  0.1991,  0.4350,  0.2467, -0.2619,  1.4727, -1.1721,\n",
      "         -0.2188, -2.4163, -1.9404, -1.1700, -0.2130, -0.1646,  0.4267,  0.8480,\n",
      "         -1.1577,  0.2477, -0.0605, -0.3608,  1.7629,  0.2176,  1.3315,  0.8754,\n",
      "          0.9276,  0.4737, -2.7118,  0.3294, -0.0211, -0.2650, -0.6659,  0.4246,\n",
      "          0.3246, -2.1494,  0.7301, -0.4359,  1.8510, -0.3716, -0.6247,  0.5450]])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(1, 1, 64)\n",
    "print(data.size())\n",
    "\n",
    "# With default parameters\n",
    "m = nn.Flatten()\n",
    "print(type(m))\n",
    "output = m(data)\n",
    "print(output.size(), output)\n",
    "\n",
    "# With non-default parameters\n",
    "m = nn.Flatten(0, 2)\n",
    "output = m(data)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n4nw95ykABAX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import os, random, pickle\n",
    "from datetime import date\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from lib.agent_utils import Agent\n",
    "\n",
    "# class Agent(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Agent, self).__init__()\n",
    "#         # 사전 학습된 MobileNetV3 Small 모델을 백본으로 로드\n",
    "#         # self.backbone = models.mobilenet_v3_small()\n",
    "#         self.backbone = models.mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "#         # MobileNetV3의 첫 번째 Conv 레이어 stride 수정 (32x32 입력에 맞게)\n",
    "#         # 원본: stride=2, 수정: stride=1 -> 작은 이미지의 공간 정보 손실 최소화\n",
    "#         # model.features(여기서는 models.mobilenet_v3_small.features)는 일반적으로 신경망 모델의 최종 예측을 수행하는 마지막 분류 계층 또는 완전 연결 계층(FC layer)을 제외한 나머지 특징 추출(feature extraction) 계층(들)을 나타낸다\n",
    "#         # self.backbone.features[0][0]는 Conv2dNormActivation 모듈 내의 Conv2d를 가리킨다\n",
    "#         # 아래와 같은 접근으로 특정 계층의 설정을 변경할 수 있다\n",
    "#         # self.backbone.features[0][0].stride = (1, 1)\n",
    "        \n",
    "#         # backbone 모델 내의 모든 컨벌루션 계층의 stride를 (1, 1)로 변경한다\n",
    "#         for _, module in self.backbone.named_modules():\n",
    "#             if isinstance(module, torch.nn.Conv2d):\n",
    "#                 module.stride = (1, 1)\n",
    "\n",
    "#         # MobileNetV3의 분류기(classifier)를 새로운 FC 레이어로 교체\n",
    "#         # MobileNetV3-Small의 분류기 입력 특성 수는 576\n",
    "#         # model.classifier(여기서는 models.mobilenet_v3_small.classifier)는 일반적으로 분류를 수행하는 신경망 모델의 마지막 계층(final layer)를 가리킨다\n",
    "#         # in_feature는 현재 계층으로 전달된 입력의 수(정확히는 입력 벡터의 차원)를 의미한다\n",
    "#         # self.backbone.classifier[-1]는 mobilenet_v3_small의 최종 예측을 수행하는 classifier의 마지막 Linear 계층을 가리킨다\n",
    "#         in_features = self.backbone.classifier[0].in_features # -1이 아니라 0이 맞는 것으로 보인다\n",
    "#         self.backbone.classifier = nn.Sequential(\n",
    "#             nn.Linear(in_features, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(1024, 4) # 4개의 행동에 대한 Q-value 출력\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # 입력 데이터의 형태 : (N, C, H, W)\n",
    "#         # N : batch_size\n",
    "#         # C : channel\n",
    "#         # H : height\n",
    "#         # W : width\n",
    "        \n",
    "#         return self.backbone(x)\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque([], maxlen=buffer_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        return\n",
    "\n",
    "    # def extend(self, transition, duplicate=100):\n",
    "    #     self.buffer.extend([transition for _ in range(duplicate)])\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, train=True, buffer_size=2000, update_freq=500):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_net = Agent().to(self.device) # 주 신경망 생성\n",
    "        \n",
    "        if train:\n",
    "            self.batch_size = 512 # available: 256, 512\n",
    "            self.gamma = 0.99 # 할인율\n",
    "            self.lr = 3e-3 # 학습률\n",
    "            \n",
    "            # 아래는 입실론 탐욕 정책 관련 변수\n",
    "            self.eps_upper = 0.9\n",
    "            self.eps_lower = 0.1\n",
    "            self.eps_rate = 10000 # (지수 함수의)x축 눈금 간격(=1)을 몇 개의 작은 부분으로 분할 것인지 설정\n",
    "            \n",
    "            # 아래는 경험 재현 기법 구현을 위한 변수\n",
    "            # Replay Memory에 transition을 저장한다\n",
    "            # transition : (cur_state, action, next_state, reward, terminated) 형태의 데이터\n",
    "            self.replay_memory = ReplayMemory(buffer_size)\n",
    "            \n",
    "            # 목표 신경망(target network) 생성\n",
    "            self.update_freq = update_freq # Target 네트워크 업데이트 주기\n",
    "            self.target_net = Agent().to(self.device)\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict()) # 목표 신경망 생성 및 주 신경망의 파라미터를 목표 신경망에 불러오기\n",
    "            self.target_net.eval() # target network는 학습하지 않는다\n",
    "\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "            self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.lr) # 옵티마이저\n",
    "            self.scaler = torch.GradScaler(self.device) # AMP를 위한 스케일러\n",
    "        \n",
    "    def eps_decay(self, ep_cnt_in_env):\n",
    "        \"\"\" 입실론 쇠퇴(decay) 구현 \"\"\"\n",
    "        \n",
    "        # epsilon decay를 통해 입실론 값은 학습이 진행될수록 점차 감소한다\n",
    "        # \"self.eps_upper - self.eps_lower\"의 일정 비율(부분)을 self.eps_lower에 반영하여 최종 eps_threshold를 계산한다\n",
    "        # np.exp는 지수 함수를 계산하는 함수로, 지수 함수를 사용하는 이유는 지수 함수의 y값 범위 중 (0, 1]를 사용하여 \"self.eps_upper - self.eps_lower\"의 몇 퍼센트를 반영하여 최종 입실론 값을 만들어 낼 것인지 결정하기 위함이다\n",
    "        return self.eps_lower + (self.eps_upper - self.eps_lower) * np.exp(-1. * ep_cnt_in_env / self.eps_rate) # 입실론 계산\n",
    "\n",
    "    def select_action(self, cur_state, eps_threshold):\n",
    "        \"\"\" 입실론 탐욕 정책(Epsilon-Greedy Policy)에 따라 행동을 선택 \"\"\"\n",
    "\n",
    "        # 탐험(Exploration): 무작위로 행동 선택\n",
    "        if random.random() < eps_threshold: # random.random()은 [0.0, 1.0) 범위의 부동 소수점 값을 반환한다\n",
    "            return random.randint(0, 3) # 스칼라; [0, 3] 구간에서 무작위로 정수 하나를 반환한다    \n",
    "        # 활용(Exploitation): 학습된 정책에 따라 최적의 행동 선택\n",
    "        else:\n",
    "            self.q_net.eval() # 에이전트를 추론 모드로 전환한다\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_net(torch.from_numpy(np.expand_dims(cur_state, axis=0)).to(self.device).float())\n",
    "            return q_values.argmax(dim=1).item() # 스칼라\n",
    "\n",
    "    def optimize_model(self, cur_ep_step):\n",
    "        \"\"\" Replay Buffer에서 batch_size 만큼 무작위 샘플링한 데이터셋으로 모델 학습을 수행한다 \"\"\"\n",
    "        \n",
    "        # Replay Buffer 내 저장된 데이터(transition)의 수가 batch_size 보다 크면 학습을 수행한다\n",
    "        if len(self.replay_memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.q_net.train() # 에이전트를 훈련 모드로 전환한다\n",
    "        \n",
    "        # Replay Buffer에서 batch_size 만큼의 데이터를 무작위로 추출한다\n",
    "        transition_set = self.replay_memory.sample(self.batch_size)\n",
    "        \n",
    "        # transitions에서 개별 데이터 추출 및 데이터 셋 구성\n",
    "        # transition : (cur_state, action, next_state, reward, terminated) 형태의 데이터\n",
    "        # cur_state : (3, 32, 32) 형태의 ndarray\n",
    "        # action : 스칼라\n",
    "        # next_state : (3, 32, 32) 형태의 ndarray\n",
    "        # reward : 스칼라\n",
    "        # terminated : True or False(bool형 데이터로 텐서가 아니다)\n",
    "        cur_state_batch = torch.from_numpy(np.stack([transition[0] for transition in transition_set], axis=0)).to(self.device).float() # cur_state; (batch_size, 3, 32, 32) 형태의 텐서\n",
    "        action_batch = torch.tensor([transition[1] for transition in transition_set], device=self.device, dtype=torch.long).reshape((-1, 1)) # action; (batch_size, 1) 형태의 텐서\n",
    "        next_state_batch = torch.from_numpy(np.stack([transition[2] for transition in transition_set], axis=0)).to(self.device).float() # next_state; (batch_size, 3, 32, 32) 형태의 텐서\n",
    "        reward_batch = torch.tensor([transition[3] for transition in transition_set], device=self.device, dtype=torch.float) # reward; (batch_size) 형태의 텐서\n",
    "        terminated_batch = torch.tensor([transition[4] for transition in transition_set], device=self.device, dtype=torch.bool) # terminated; (batch_size) 형태의 텐서\n",
    "        \n",
    "        # AMP(Automatic Mixed Precision) 사용\n",
    "        with torch.autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "            # 예측 Q-value 계산: Q(s, a)(예측값에 해당)\n",
    "            pred_q_values = self.q_net(cur_state_batch).gather(dim=1, index=action_batch) # (batchs_size, 1) 형태의 텐서가 반환된다\n",
    "        \n",
    "            # 타겟 Q-value 계산: r + γ * max_a' Q_target(s', a')(실제값에 해당)\n",
    "            with torch.no_grad():\n",
    "                # torch.max()는 dim을 기준으로 가장 큰 값과 이 값을 갖는 원소의 인덱스를 튜플로 묶어서 반환한다\n",
    "                next_q_values = self.target_net(next_state_batch).max(dim=1)[0] # (batchs_size) 형태의 텐서가 반환된다\n",
    "            next_q_values[terminated_batch] = 0.0 # next_state가 에피소드 종료에 해당한다면 더 이상 행동을 취할 수 없으므로 \"max_a' Q_target(s', a')\"를 0으로 한다\n",
    "            target_q_values = reward_batch + (self.gamma * next_q_values) # (batchs_size) 형태의 텐서가 반환된다\n",
    "            \n",
    "            # Loss 계산\n",
    "            # Q-러닝 공식: Q(s, a) = Q(s, a) + α(r + γ * max_a' Q_target(s', a') - Q(s, a))\n",
    "            # loss = F.smooth_l1_loss(pred_q_values, target_q_values.unsqueeze(dim=1).detach()) # Smooth L1 Loss는 L1 Loss와 L2 Loss를 합친 것이다\n",
    "            loss = self.loss_fn(pred_q_values, target_q_values.unsqueeze(dim=1).detach())\n",
    "        \n",
    "        # 역전파\n",
    "        self.optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # self.optimizer.step()\n",
    "        '''\n",
    "        self.scaler.scale(loss).backward(): Loss를 스케일한 뒤 역전파를 수행하여, Loss와 동일하게 스케일된 그래디언트를 도출한다\n",
    "        - 이는 fp16이 표현 가능한 범위안에서 그래디언트가 계산될 수 있도록 만들어, fp32에서 fp16으로 정밀도를 낮추었을 때 계산된 대부분의 그래디언트가 0으로 처리되는 것을 방지한다\n",
    "        - torch.autocast() 하에서의 역전파 과정은 추천하지 않는다\n",
    "        - 역전파 연산은 torch.autocast에 의해 순전파 연산에 적용된 dtype과 동일한 dypte을 적용한다\n",
    "        self.scaler.step(self.optimizer): Loss 스케일의 영향으로 동일하게 스케일된 그래디언트들을 fp32로 변환한 후, 스케일과 동일한 수치로 언스케일한 다음 매개변수 갱신을 수행한다\n",
    "        - 언스케일된 그래디언트들이 infs 또는 NaNs를 포함하고 있지 않다면, step()이 호출되어 매개변수 갱신을 수행한다\n",
    "        - 만약 infs 또는 NaNs를 포함한다면, step() 호출은 생략된다\n",
    "        self.scaler.update(): 다음 iteration을 위해 scaler의 scale을 갱신한다\n",
    "        \n",
    "        [참고]\n",
    "        1. https://computing-jhson.tistory.com/36\n",
    "        2. https://docs.pytorch.org/docs/stable/amp.html\n",
    "        '''\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        # target network 갱신\n",
    "        if cur_ep_step % self.update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "    \n",
    "    def train(self, env, pathes=None, max_env_creation_cnt=10, max_ep_win_in_env=100):\n",
    "        \"\"\" DQN 에이전트의 학습을 수행한다 \"\"\"\n",
    "        \n",
    "        env_creation_cnt = 0 # 환경 생성 횟수\n",
    "        whole_env_rewards = [] # 전체 환경에서 수행된 모든 에피소드들의 최종 보상 모음\n",
    "        info = None # 불러올 체크포인트 당시 환경에 대한 정보 모음\n",
    "\n",
    "        # 체크포인트 불러오기\n",
    "        if pathes is not None:\n",
    "            info, env = self.load(self.q_net, pathes)\n",
    "            max_env_creation_cnt, env_creation_cnt, max_ep_win_in_env = info[:3]\n",
    "        \n",
    "        # 에피소드 승리 횟수가 max_ep_win_in_env에 이를 때 때까지 동일 환경에서 학습을 반복한다\n",
    "        while env_creation_cnt != max_env_creation_cnt:\n",
    "            cur_ep_step = 0 # 현재 에피소드 내 스텝 경과\n",
    "            ep_win_in_env = 0 # 현재 환경에서 에피소드 승리 횟수\n",
    "            ep_cnt_in_env = 0 # 현재 환경에서 에피소드 수행 횟수\n",
    "            cur_env_rewards = [] # 현재 환경에서 수행된 모든 에피소드들의 최종 보상 모음\n",
    "            ep_step_record = deque([], maxlen=30) # 현재 환경에서 수행된 모든 에피소드들의 총 스텝 모음\n",
    "            \n",
    "            # 환경 생성 횟수가 목표 환경 생성 횟수의 1/2 지점에 도달했을 경우 self.eps_upper/self.eps_lower를 재설정한다\n",
    "            if env_creation_cnt == int(max_env_creation_cnt // 2):\n",
    "                self.eps_upper, self.eps_lower = 0.7, 0.05\n",
    "\n",
    "            if info is not None:\n",
    "                ep_win_in_env, ep_cnt_in_env = info[3:]\n",
    "                eps_threshold = self.eps_decay(ep_cnt_in_env) # 입실론 계산\n",
    "                cur_state = env.rest(fixed=True) / 4 # 환경 초기화\n",
    "                info = None\n",
    "            else:\n",
    "                eps_threshold = self.eps_upper # 초기 입실론 설정\n",
    "                cur_state = env.reset(fixed=False) / 4 # 새로운 환경 생성 및 초기화\n",
    "                env_creation_cnt += 1\n",
    "                print(f\"Create new Env : ({env_creation_cnt}/{max_env_creation_cnt}), eps_upper/eps_lower : {self.eps_upper}/{self.eps_lower}\")\n",
    "                \n",
    "            # fig, ax = plt.subplots(figsize=(6, 6)) # 그림 크기는 필요에 따라 조절\n",
    "            # env.normal_view_render(ax, pause_time=0.0001)\n",
    "            \n",
    "            # 현재 환경에서 에피소드 수행\n",
    "            while True:\n",
    "                cur_ep_step += 1\n",
    "                action = self.select_action(cur_state, eps_threshold)\n",
    "                next_state, reward, terminated, passed, debug = env.step(action)\n",
    "                \n",
    "                print(f\"Current action / reward: ({action} / {reward})\")\n",
    "                # fig, ax = plt.subplots(figsize=(6, 6)) # 그림 크기는 필요에 따라 조절\n",
    "                # env.normal_view_render(ax, pause_time=0.0001)\n",
    "                \n",
    "                # 에피소드에서 승리한 경우\n",
    "                if passed:\n",
    "                    ep_win_in_env += 1\n",
    "                    \n",
    "                self.replay_memory.append((cur_state, action, next_state, reward, terminated))\n",
    "                self.optimize_model(cur_ep_step) # 에이전트 학습 및 타겟 네트워크 갱신\n",
    "                \n",
    "                # 에피소드 종료\n",
    "                if terminated:\n",
    "                    clear_output(wait=True)\n",
    "                    ep_cnt_in_env += 1\n",
    "                    cur_env_rewards.append(reward)\n",
    "                    ep_step_record.append(debug[0])\n",
    "                    print(f\"Env {env_creation_cnt}/{max_env_creation_cnt}: Episode {ep_cnt_in_env} Done!\")\n",
    "                    print(f\"Current eps_threshold: {eps_threshold} // Total win episode/Goal win episode = ({ep_win_in_env}/{max_ep_win_in_env})\")\n",
    "                    print(f\"Tracking episode step(currrent 20 episode): {list(ep_step_record)}\")\n",
    "                    print(f\"Moving average of episode step: {sum(ep_step_record) / 20}\")\n",
    "                    print(f\"Best action sequence(len/seq): {len(debug[1])}/{debug[1]}\")\n",
    "\n",
    "                    # 에이전트 체크포인트와 환경 저장\n",
    "                    self.save_data(max_env_creation_cnt, env_creation_cnt, max_ep_win_in_env, ep_win_in_env, ep_cnt_in_env, env)\n",
    "                    \n",
    "                    # 현재 환경에서 에피소드 승리 횟수가 목표 승리 횟수에 도달한 경우\n",
    "                    if ep_win_in_env == max_ep_win_in_env:\n",
    "                        self.save(max_env_creation_cnt, env_creation_cnt, max_ep_win_in_env, ep_win_in_env, ep_cnt_in_env) # 에이전트 체크포인트만 저장                        \n",
    "                        whole_env_rewards.append(cur_env_rewards) # 현재 환경에서 수행한 모든 에피소드의 최종 보상값 모음을 저장한다\n",
    "                        break # 현재 환경에서의 학습을 종료하고 다음 환경으로 넘어간다\n",
    "                    # 현재 환경에서 에피소드 승리 횟수가 목표 승리 횟수에 도달하지 못한 경우\n",
    "                    else:\n",
    "                        cur_ep_step = 0\n",
    "                        eps_threshold = self.eps_decay(ep_cnt_in_env)\n",
    "                        # 목표 승리 횟수에 도달할 때까지 현재 환경에서의 학습을 반복한다\n",
    "                        cur_state = env.reset(fixed=True) / 4 # 현재 환경을 초기화하고 에피소드를 다시 수행한다\n",
    "                        continue\n",
    "        \n",
    "                cur_state = next_state\n",
    "        \n",
    "        print(\"===== Train Done =====\")\n",
    "        \n",
    "        return whole_env_rewards\n",
    "\n",
    "    def inference(self, env, init_state, checkpoint):\n",
    "        \"\"\" 학습된 에이전트로 특정 환경에서 에피소드를 수행하고 그 결과를 리플레이 형태로 반환한다 \"\"\"\n",
    "\n",
    "        self.q_net.load_state_dict(torch.load(checkpoint, weights_only=True)) # 에이전트 체크포인트 불러오기\n",
    "        self.q_net.eval() # 에이전트를 평가 모드로 설정\n",
    "\n",
    "        cur_state = init_state\n",
    "        replay = np.expand_dims(cur_state, axis=0) # 현재 상태를 기록한다\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 6)) # 그림 크기는 필요에 따라 조절\n",
    "        env.normal_view_render(ax, pause_time=0.0001)\n",
    "\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_net(torch.from_numpy(np.expand_dims(cur_state, axis=0)).to(self.device).float())\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "            next_state, reward, terminated = env.step(action)\n",
    "            print(f\"Current action / reward: {action} / {reward}\")\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(6, 6)) # 그림 크기는 필요에 따라 조절\n",
    "            env.normal_view_render(ax, pause_time=0.0001)\n",
    "            \n",
    "            # 다음 상태(next_state)를 element-wise product로 결합하여 저장\n",
    "            replay = np.concatenate((replay, np.expand_dims(next_state, axis=0)), axis=0)\n",
    "\n",
    "            # 에피소드 종료\n",
    "            if terminated:\n",
    "                break\n",
    "            \n",
    "            cur_state = next_state\n",
    "\n",
    "        print(\"===== Inference Done =====\")\n",
    "\n",
    "        return replay\n",
    "\n",
    "    def save_data(self, max_env_creation_cnt, env_creation_cnt, max_ep_win_in_env, ep_win_in_env, ep_cnt_in_env, env=None):\n",
    "        \"\"\" 에이전트 체크포인트와 환경 저장 \"\"\"\n",
    "\n",
    "        # 날짜 기록\n",
    "        dates = str(date.today())\n",
    "\n",
    "        # 환경 저장\n",
    "        if (env is not None) and (ep_cnt_in_env % 300 == 0):\n",
    "            with open(os.path.join(os.getcwd(), \"checkpoints\", f\"env_{max_env_creation_cnt}_{env_creation_cnt}_{max_ep_win_in_env}_{ep_win_in_env}_{ep_cnt_in_env}_{dates}.obj\"), \"wb\") as f:\n",
    "                pickle.dump(env, f)\n",
    "        \n",
    "        # 에이전트 체크포인트 저장\n",
    "        torch.save(self.q_net.state_dict(), \\\n",
    "               os.path.join(os.getcwd(), \"checkpoints\", f\"checkpoint_{max_env_creation_cnt}_{env_creation_cnt}_{max_ep_win_in_env}_{ep_win_in_env}_{ep_cnt_in_env}_{dates}.pth\"))\n",
    "        return\n",
    "\n",
    "    def load(self, model, pathes):\n",
    "        # pathes : [checkpoint_path, env_path] or [checkpoint_path, None]\n",
    "        # 모델 체크포인트 불러오기\n",
    "        model.load_state_dict(torch.load(pathes[0], weights_only=True))\n",
    "        \n",
    "        # max_env_creation_cnt, env_creation_cnt, max_ep_win_in_env, ep_win_in_env, ep_cnt_in_env 정보 불러오기\n",
    "        info = [int(item) for item in pathes[0].split('\\\\')[-1].split('_')[1:6]]\n",
    "\n",
    "        if pathes[1] is not None:\n",
    "            with open(pathes[1], \"rb\") as f:\n",
    "                env = pickle.load(f)\n",
    "        \n",
    "        return info, env\n",
    "        \n",
    "    def plot_mv_avgs(self, rewards_set, window_size=10):\n",
    "        \"\"\" 학습 결과에 기반하여 이동 평균을 계산하고 시각화한다 \"\"\"\n",
    "\n",
    "        rewards_set = np.numpy(rewards_set)\n",
    "        mv_avg_set = []\n",
    "\n",
    "        for rewards in rewards_set:\n",
    "            mv_avg_set.append([(rewards[i:i+window_size] @ np.ones(10)) / window_size for i in range(len(rewards) - window_size + 1)])\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.title(\"Rewards Moving Average\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        for idx, result in enumerate(mv_avg_set):\n",
    "            plt.plot(result, label=f\"Env {idx + 1}\", alpha=0.6, linewidth=2)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env 1/10: Episode 13979 Done!\n",
      "Current eps_threshold: 0.2977120595718392 // Total win episode/Goal win episode = (1/50)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m env = PathFindEnv(height=\u001b[32m32\u001b[39m, width=\u001b[32m32\u001b[39m)\n\u001b[32m      4\u001b[39m dqn = DQN(train=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m result = \u001b[43mdqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_creation_cnt\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ep_win_in_env\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 253\u001b[39m, in \u001b[36mDQN.train\u001b[39m\u001b[34m(self, env, pathes, max_env_creation_cnt, max_ep_win_in_env)\u001b[39m\n\u001b[32m    251\u001b[39m ep_step_record.append(debug[\u001b[32m0\u001b[39m])\n\u001b[32m    252\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnv \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_creation_cnt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_env_creation_cnt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_cnt_in_env\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Done!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCurrent eps_threshold: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43meps_threshold\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m // Total win episode/Goal win episode = (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mep_win_in_env\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmax_ep_win_in_env\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTracking episode step(currrent 20 episode): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(ep_step_record)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    255\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMoving average of episode step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(ep_step_record)\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\gradenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3045\u001b[39m, in \u001b[36mInteractiveShell._tee.<locals>.write\u001b[39m\u001b[34m(data, *args, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite\u001b[39m(data, *args, **kwargs):\n\u001b[32m   3044\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write data to both the original destination and the capture dictionary.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m     result = \u001b[43moriginal_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   3047\u001b[39m         [\n\u001b[32m   3048\u001b[39m             \u001b[38;5;28mself\u001b[39m.display_pub.is_publishing,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3051\u001b[39m         ]\n\u001b[32m   3052\u001b[39m     ):\n\u001b[32m   3053\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\gradenv\\Lib\\site-packages\\ipykernel\\iostream.py:655\u001b[39m, in \u001b[36mOutStream.write\u001b[39m\u001b[34m(self, string)\u001b[39m\n\u001b[32m    647\u001b[39m                     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    649\u001b[39m             \u001b[38;5;28mself\u001b[39m.session.send(\n\u001b[32m    650\u001b[39m                 \u001b[38;5;28mself\u001b[39m.pub_thread,\n\u001b[32m    651\u001b[39m                 msg,\n\u001b[32m    652\u001b[39m                 ident=\u001b[38;5;28mself\u001b[39m.topic,\n\u001b[32m    653\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, string: \u001b[38;5;28mstr\u001b[39m) -> Optional[\u001b[38;5;28mint\u001b[39m]:  \u001b[38;5;66;03m# type:ignore[override]\u001b[39;00m\n\u001b[32m    656\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write to current stream after encoding if necessary\u001b[39;00m\n\u001b[32m    657\u001b[39m \n\u001b[32m    658\u001b[39m \u001b[33;03m    Returns\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    662\u001b[39m \n\u001b[32m    663\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    664\u001b[39m     parent = \u001b[38;5;28mself\u001b[39m.parent_header\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from lib.env_generator7 import PathFindEnv\n",
    "\n",
    "env = PathFindEnv(height=32, width=32)\n",
    "dqn = DQN(train=True)\n",
    "\n",
    "result = dqn.train(env, max_env_creation_cnt=10, max_ep_win_in_env=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env 1/10: Episode 14849 Done!\n",
      "Current eps_threshold: 0.28123811653996234 // Total win episode/Goal win episode = (0/50)\n",
      "Tracking episode step(currrent 20 episode): [13, 9, 15, 43, 34, 5, 3, 6, 22, 42, 40, 40, 5, 3, 4, 3, 3, 8, 32, 6, 20, 7, 3, 14, 4, 7, 28, 11, 10, 28]\n",
      "Moving average of episode step: 23.4\n",
      "Best action sequence(len/seq): 39/[1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 3]\n",
      "|| Src2Dest full distance/Current remain disatance: (37/36) // Radius of levels/Progresss: ([ 7 14 22]/[-29 -22 -14]) ||\n",
      "Current action / reward: (1 / 4.0)\n",
      "|| Src2Dest full distance/Current remain disatance: (37/35) // Radius of levels/Progresss: ([ 7 14 22]/[-28 -21 -13]) ||\n",
      "Current action / reward: (3 / 4.0)\n",
      "|| Src2Dest full distance/Current remain disatance: (37/36) // Radius of levels/Progresss: ([ 7 14 22]/[-29 -22 -14]) ||\n",
      "Current action / reward: (0 / 0.0)\n",
      "|| Src2Dest full distance/Current remain disatance: (37/35) // Radius of levels/Progresss: ([ 7 14 22]/[-28 -21 -13]) ||\n",
      "Current action / reward: (1 / 8.0)\n",
      "|| Src2Dest full distance/Current remain disatance: (37/34) // Radius of levels/Progresss: ([ 7 14 22]/[-27 -20 -12]) ||\n",
      "Current action / reward: (1 / 4.0)\n",
      "|| Src2Dest full distance/Current remain disatance: (37/33) // Radius of levels/Progresss: ([ 7 14 22]/[-26 -19 -11]) ||\n",
      "Current action / reward: (1 / 4.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m env = PathFindEnv(height=\u001b[32m32\u001b[39m, width=\u001b[32m32\u001b[39m)\n\u001b[32m      4\u001b[39m dqn = DQN(train=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m result = \u001b[43mdqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_creation_cnt\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ep_win_in_env\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 239\u001b[39m, in \u001b[36mDQN.train\u001b[39m\u001b[34m(self, env, checkpoint, max_env_creation_cnt, max_ep_win_in_env)\u001b[39m\n\u001b[32m    236\u001b[39m     ep_win_in_env += \u001b[32m1\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28mself\u001b[39m.replay_memory.append((cur_state, action, next_state, reward, terminated))\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_ep_step\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 에이전트 학습 및 타겟 네트워크 갱신\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# 에피소드 종료\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36mDQN.optimize_model\u001b[39m\u001b[34m(self, cur_ep_step)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[33;03mself.scaler.scale(loss).backward(): Loss를 스케일한 뒤 역전파를 수행하여, Loss와 동일하게 스케일된 그래디언트를 도출한다\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[33;03m- 이는 fp16이 표현 가능한 범위안에서 그래디언트가 계산될 수 있도록 만들어, fp32에서 fp16으로 정밀도를 낮추었을 때 계산된 대부분의 그래디언트가 0으로 처리되는 것을 방지한다\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    181\u001b[39m \u001b[33;03m2. https://docs.pytorch.org/docs/stable/amp.html\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.scale(loss).backward()\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.update()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# target network 갱신\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\gradenv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\gradenv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\gradenv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from lib.env_generator6 import PathFindEnv\n",
    "\n",
    "env = PathFindEnv(height=32, width=32)\n",
    "dqn = DQN(train=True)\n",
    "\n",
    "result = dqn.train(env, max_env_creation_cnt=10, max_ep_win_in_env=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env 1/10: Episode 81608 Done!\n",
      "Current eps_threshold: 0.453732765263685 // Total win episode/Goal win episode = (0/50)\n",
      "Tracking episode step(currrent 20 episode): [2, 3, 1, 10, 1, 3, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 4, 5, 1, 1, 1, 1]\n",
      "Moving average of episode step: 2.75\n",
      "Src2Dest full distance/Agent step upper limit/Current remain disatance: (52/156/53) // Radius of levels/Progresss: ([ 5 15 25]/[-48 -38 -28])\n",
      "Current action / reward: (2 / 1.5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m env = PathFindEnv(height=\u001b[32m32\u001b[39m, width=\u001b[32m32\u001b[39m)\n\u001b[32m      4\u001b[39m dqn = DQN(train=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m result = \u001b[43mdqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_creation_cnt\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ep_win_in_env\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 238\u001b[39m, in \u001b[36mDQN.train\u001b[39m\u001b[34m(self, env, checkpoint, max_env_creation_cnt, max_ep_win_in_env)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m passed:\n\u001b[32m    236\u001b[39m     ep_win_in_env += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_memory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.optimize_model(cur_ep_step) \u001b[38;5;66;03m# 에이전트 학습 및 타겟 네트워크 갱신\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# 에피소드 종료\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mReplayMemory.append\u001b[39m\u001b[34m(self, transition)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.buffer)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, transition):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mself\u001b[39m.buffer.append(transition)\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from lib.env_generator5 import PathFindEnv\n",
    "\n",
    "env = PathFindEnv(height=32, width=32)\n",
    "dqn = DQN(train=True)\n",
    "\n",
    "result = dqn.train(env, max_env_creation_cnt=10, max_ep_win_in_env=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env 1/10: Episode 371 Done!\n",
      "Current eps_threshold: 0.8429373550730298 // Total win episode / Goal win episode = (0/50)\n",
      "Collision rate(previous/current): (22/0) // Tendency of collision rate: [1, 8, 32, 2, 10, 16, 34, 1, 26, 13, 4, 28, 28, 30, 0, 34, 4, 13, 9, 11, 29, 7, 12, 0, 1, 2, 3, 24, 22, 0]\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/40) // Radius of levels / Progresss: ([ 6 13 20]/[-34 -27 -20])\n",
      "Current action / reward: (0 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/41) // Radius of levels / Progresss: ([ 6 13 20]/[-35 -28 -21])\n",
      "Current action / reward: (1 / 1.5)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/40) // Radius of levels / Progresss: ([ 6 13 20]/[-34 -27 -20])\n",
      "Current action / reward: (3 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/39) // Radius of levels / Progresss: ([ 6 13 20]/[-33 -26 -19])\n",
      "Current action / reward: (0 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/38) // Radius of levels / Progresss: ([ 6 13 20]/[-32 -25 -18])\n",
      "Current action / reward: (0 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/37) // Radius of levels / Progresss: ([ 6 13 20]/[-31 -24 -17])\n",
      "Current action / reward: (0 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/38) // Radius of levels / Progresss: ([ 6 13 20]/[-32 -25 -18])\n",
      "Current action / reward: (1 / 1.5)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/39) // Radius of levels / Progresss: ([ 6 13 20]/[-33 -26 -19])\n",
      "Current action / reward: (1 / 1.5)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/40) // Radius of levels / Progresss: ([ 6 13 20]/[-34 -27 -20])\n",
      "Current action / reward: (2 / 1.5)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/39) // Radius of levels / Progresss: ([ 6 13 20]/[-33 -26 -19])\n",
      "Current action / reward: (3 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/40) // Radius of levels / Progresss: ([ 6 13 20]/[-34 -27 -20])\n",
      "Current action / reward: (1 / 1.5)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/39) // Radius of levels / Progresss: ([ 6 13 20]/[-33 -26 -19])\n",
      "Current action / reward: (3 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/38) // Radius of levels / Progresss: ([ 6 13 20]/[-32 -25 -18])\n",
      "Current action / reward: (0 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/39) // Radius of levels / Progresss: ([ 6 13 20]/[-33 -26 -19])\n",
      "Current action / reward: (1 / 1.5)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/40) // Radius of levels / Progresss: ([ 6 13 20]/[-34 -27 -20])\n",
      "Current action / reward: (2 / 1.5)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/39) // Radius of levels / Progresss: ([ 6 13 20]/[-33 -26 -19])\n",
      "Current action / reward: (3 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/38) // Radius of levels / Progresss: ([ 6 13 20]/[-32 -25 -18])\n",
      "Current action / reward: (3 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/39) // Radius of levels / Progresss: ([ 6 13 20]/[-33 -26 -19])\n",
      "Current action / reward: (1 / 1.5)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/40) // Radius of levels / Progresss: ([ 6 13 20]/[-34 -27 -20])\n",
      "Current action / reward: (2 / 1.5)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/39) // Radius of levels / Progresss: ([ 6 13 20]/[-33 -26 -19])\n",
      "Current action / reward: (3 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/40) // Radius of levels / Progresss: ([ 6 13 20]/[-34 -27 -20])\n",
      "Current action / reward: (1 / 1.5)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/39) // Radius of levels / Progresss: ([ 6 13 20]/[-33 -26 -19])\n",
      "Current action / reward: (3 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/38) // Radius of levels / Progresss: ([ 6 13 20]/[-32 -25 -18])\n",
      "Current action / reward: (3 / 5.0)\n",
      "Src2Dest full distance / Agent step upper limit / Current remain disatance: (41/123/39) // Radius of levels / Progresss: ([ 6 13 20]/[-33 -26 -19])\n",
      "Current action / reward: (2 / 1.5)\n"
     ]
    }
   ],
   "source": [
    "from lib.env_generator4 import PathFindEnv\n",
    "\n",
    "env = PathFindEnv(height=32, width=32)\n",
    "dqn = DQN(train=True)\n",
    "\n",
    "result = dqn.train(env, max_env_creation_cnt=10, max_ep_win_in_env=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env 1/10: Episode 50268 Done!, Current eps_threshold: 0.3927361606265315, Total win episode/Goal win episode = (0/50)\n",
      "Src2Dest full distance / Current remain disatance: (35 / 34)\n",
      "Current action: 1, Current reward: 0.056\n",
      "Src2Dest full distance / Current remain disatance: (35 / 35)\n",
      "Current action: 0, Current reward: -0.095\n",
      "Src2Dest full distance / Current remain disatance: (35 / 34)\n",
      "Current action: 1, Current reward: 0.056\n",
      "Src2Dest full distance / Current remain disatance: (35 / 33)\n",
      "Current action: 1, Current reward: 0.057\n",
      "Src2Dest full distance / Current remain disatance: (35 / 32)\n",
      "Current action: 1, Current reward: 0.058\n",
      "Src2Dest full distance / Current remain disatance: (35 / 33)\n",
      "Current action: 0, Current reward: -0.093\n",
      "Src2Dest full distance / Current remain disatance: (35 / 32)\n",
      "Current action: 1, Current reward: 0.058\n",
      "Src2Dest full distance / Current remain disatance: (35 / 33)\n",
      "Current action: 0, Current reward: -0.093\n",
      "Src2Dest full distance / Current remain disatance: (35 / 32)\n",
      "Current action: 1, Current reward: 0.058\n",
      "Src2Dest full distance / Current remain disatance: (35 / 33)\n",
      "Current action: 0, Current reward: -0.093\n",
      "Src2Dest full distance / Current remain disatance: (35 / 32)\n",
      "Current action: 2, Current reward: 0.058\n",
      "Src2Dest full distance / Current remain disatance: (35 / 31)\n",
      "Current action: 1, Current reward: 0.059000000000000004\n",
      "Src2Dest full distance / Current remain disatance: (35 / 30)\n",
      "Current action: 1, Current reward: 0.060000000000000005\n",
      "Src2Dest full distance / Current remain disatance: (35 / 29)\n",
      "Current action: 1, Current reward: 0.061000000000000006\n",
      "Src2Dest full distance / Current remain disatance: (35 / 28)\n",
      "Current action: 1, Current reward: 0.062000000000000006\n",
      "Src2Dest full distance / Current remain disatance: (35 / 27)\n",
      "Current action: 1, Current reward: 0.06300000000000001\n",
      "Src2Dest full distance / Current remain disatance: (35 / 26)\n",
      "Current action: 1, Current reward: 0.06400000000000002\n",
      "Src2Dest full distance / Current remain disatance: (35 / 25)\n",
      "Current action: 1, Current reward: 0.06500000000000002\n",
      "Src2Dest full distance / Current remain disatance: (35 / 24)\n",
      "Current action: 1, Current reward: 0.06600000000000002\n",
      "Src2Dest full distance / Current remain disatance: (35 / 23)\n",
      "Current action: 1, Current reward: 0.06700000000000002\n",
      "Src2Dest full distance / Current remain disatance: (35 / 22)\n",
      "Current action: 1, Current reward: 0.06800000000000002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m env = PathFindEnv(height=\u001b[32m32\u001b[39m, width=\u001b[32m32\u001b[39m)\n\u001b[32m      4\u001b[39m dqn = DQN(train=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m result = \u001b[43mdqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_creation_cnt\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ep_win_in_env\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 240\u001b[39m, in \u001b[36mDQN.train\u001b[39m\u001b[34m(self, env, checkpoint, max_env_creation_cnt, max_ep_win_in_env)\u001b[39m\n\u001b[32m    238\u001b[39m     ep_win_in_env += \u001b[32m1\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.replay_memory.append((cur_state, action, next_state, reward, terminated))\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_ep_step\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 에이전트 학습 및 타겟 네트워크 갱신\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# 에피소드 종료\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mDQN.optimize_model\u001b[39m\u001b[34m(self, cur_ep_step)\u001b[39m\n\u001b[32m    134\u001b[39m transition_set = \u001b[38;5;28mself\u001b[39m.replay_memory.sample(\u001b[38;5;28mself\u001b[39m.batch_size)\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# transitions에서 개별 데이터 추출 및 데이터 셋 구성\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# transition : (cur_state, action, next_state, reward, terminated) 형태의 데이터\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# cur_state : (3, 32, 32) 형태의 ndarray\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# reward : 스칼라\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# terminated : True or False(bool형 데이터로 텐서가 아니다)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m cur_state_batch = torch.from_numpy(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransition\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransition\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransition_set\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m).to(\u001b[38;5;28mself\u001b[39m.device).float() \u001b[38;5;66;03m# cur_state; (batch_size, 3, 32, 32) 형태의 텐서\u001b[39;00m\n\u001b[32m    144\u001b[39m action_batch = torch.tensor([transition[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m transition \u001b[38;5;129;01min\u001b[39;00m transition_set], device=\u001b[38;5;28mself\u001b[39m.device, dtype=torch.long).reshape((-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)) \u001b[38;5;66;03m# action; (batch_size, 1) 형태의 텐서\u001b[39;00m\n\u001b[32m    145\u001b[39m next_state_batch = torch.from_numpy(np.stack([transition[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m transition \u001b[38;5;129;01min\u001b[39;00m transition_set], axis=\u001b[32m0\u001b[39m)).to(\u001b[38;5;28mself\u001b[39m.device).float() \u001b[38;5;66;03m# next_state; (batch_size, 3, 32, 32) 형태의 텐서\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\gradenv\\Lib\\site-packages\\numpy\\_core\\shape_base.py:467\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype, casting)\u001b[39m\n\u001b[32m    465\u001b[39m sl = (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) * axis + (_nx.newaxis,)\n\u001b[32m    466\u001b[39m expanded_arrays = [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from lib.env_generator3 import PathFindEnv\n",
    "\n",
    "env = PathFindEnv(height=32, width=32)\n",
    "dqn = DQN(train=True)\n",
    "\n",
    "result = dqn.train(env, max_env_creation_cnt=10, max_ep_win_in_env=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env 1/10: Episode 20448 Done!, Current eps_threshold: 0.20353520788564405, Total win episode/Goal win episode = (0/50)\n",
      "Current action / reward: (0 / -10.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m env = PathFindEnv(height=\u001b[32m32\u001b[39m, width=\u001b[32m32\u001b[39m)\n\u001b[32m      4\u001b[39m dqn = DQN(train=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m result = \u001b[43mdqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_creation_cnt\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ep_win_in_env\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 239\u001b[39m, in \u001b[36mDQN.train\u001b[39m\u001b[34m(self, env, checkpoint, max_env_creation_cnt, max_ep_win_in_env)\u001b[39m\n\u001b[32m    235\u001b[39m     ep_win_in_env += \u001b[32m1\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28mself\u001b[39m.replay_memory.extend((cur_state, action, next_state, reward, terminated)) \u001b[38;5;28;01mif\u001b[39;00m reward >= \u001b[32m2.0\u001b[39m \\\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.replay_memory.append((cur_state, action, next_state, reward, terminated))\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_ep_step\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 에이전트 학습 및 타겟 네트워크 갱신\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# 에피소드 종료\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36mDQN.optimize_model\u001b[39m\u001b[34m(self, cur_ep_step)\u001b[39m\n\u001b[32m    143\u001b[39m cur_state_batch = torch.from_numpy(np.stack([transition[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m transition \u001b[38;5;129;01min\u001b[39;00m transition_set], axis=\u001b[32m0\u001b[39m)).to(\u001b[38;5;28mself\u001b[39m.device).float() \u001b[38;5;66;03m# cur_state; (batch_size, 3, 32, 32) 형태의 텐서\u001b[39;00m\n\u001b[32m    144\u001b[39m action_batch = torch.tensor([transition[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m transition \u001b[38;5;129;01min\u001b[39;00m transition_set], device=\u001b[38;5;28mself\u001b[39m.device, dtype=torch.long).reshape((-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)) \u001b[38;5;66;03m# action; (batch_size, 1) 형태의 텐서\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m next_state_batch = torch.from_numpy(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransition\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransition\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransition_set\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m).to(\u001b[38;5;28mself\u001b[39m.device).float() \u001b[38;5;66;03m# next_state; (batch_size, 3, 32, 32) 형태의 텐서\u001b[39;00m\n\u001b[32m    146\u001b[39m reward_batch = torch.tensor([transition[\u001b[32m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m transition \u001b[38;5;129;01min\u001b[39;00m transition_set], device=\u001b[38;5;28mself\u001b[39m.device, dtype=torch.float) \u001b[38;5;66;03m# reward; (batch_size) 형태의 텐서\u001b[39;00m\n\u001b[32m    147\u001b[39m terminated_batch = torch.tensor([transition[\u001b[32m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m transition \u001b[38;5;129;01min\u001b[39;00m transition_set], device=\u001b[38;5;28mself\u001b[39m.device, dtype=torch.bool) \u001b[38;5;66;03m# terminated; (batch_size) 형태의 텐서\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\gradenv\\Lib\\site-packages\\numpy\\_core\\shape_base.py:467\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype, casting)\u001b[39m\n\u001b[32m    465\u001b[39m sl = (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) * axis + (_nx.newaxis,)\n\u001b[32m    466\u001b[39m expanded_arrays = [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from lib.env_generator2 import PathFindEnv\n",
    "\n",
    "env = PathFindEnv(height=32, width=32)\n",
    "dqn = DQN(train=True)\n",
    "\n",
    "result = dqn.train(env, max_env_creation_cnt=10, max_ep_win_in_env=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env 1/10: Episode 3410 Done!, Current eps_threshold: 0.35679507527004684, Total win episode/Goal win episode = (20/50)\n",
      "step upper limit: 93 / current radius of each level: [ 5 10 15 20] / current temp of check_level: [-21 -16 -11  -6]\n",
      "Current action: 0, Current reward: -1\n",
      "step upper limit: 93 / current radius of each level: [ 5 10 15 20] / current temp of check_level: [-21 -16 -11  -6]\n",
      "Current action: 0, Current reward: -1\n",
      "step upper limit: 93 / current radius of each level: [ 5 10 15 20] / current temp of check_level: [-22 -17 -12  -7]\n",
      "Current action: 0, Current reward: -1\n",
      "step upper limit: 93 / current radius of each level: [ 5 10 15 20] / current temp of check_level: [-22 -17 -12  -7]\n",
      "Current action: 0, Current reward: -1\n",
      "step upper limit: 93 / current radius of each level: [ 5 10 15 20] / current temp of check_level: [-22 -17 -12  -7]\n",
      "Current action: 0, Current reward: -1\n",
      "step upper limit: 93 / current radius of each level: [ 5 10 15 20] / current temp of check_level: [-22 -17 -12  -7]\n",
      "Current action: 1, Current reward: -1\n",
      "step upper limit: 93 / current radius of each level: [ 5 10 15 20] / current temp of check_level: [-22 -17 -12  -7]\n",
      "Current action: 1, Current reward: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m env = PathFindEnv(height=\u001b[32m32\u001b[39m, width=\u001b[32m32\u001b[39m)\n\u001b[32m      4\u001b[39m dqn = DQN(train=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m result = \u001b[43mdqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_creation_cnt\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ep_win_in_env\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 281\u001b[39m, in \u001b[36mDQN.train\u001b[39m\u001b[34m(self, env, checkpoint, max_env_creation_cnt, max_ep_win_in_env)\u001b[39m\n\u001b[32m    279\u001b[39m     ep_win_in_env += \u001b[32m1\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28mself\u001b[39m.replay_memory.append((cur_state, action, next_state, reward, terminated))\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_ep_step\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 에이전트 학습 및 타겟 네트워크 갱신\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# 에피소드 종료\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 225\u001b[39m, in \u001b[36mDQN.optimize_model\u001b[39m\u001b[34m(self, cur_ep_step)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[33;03mself.scaler.scale(loss).backward(): Loss를 스케일한 뒤 역전파를 수행하여, Loss와 동일하게 스케일된 그래디언트를 도출한다\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m- 이는 fp16이 표현 가능한 범위안에서 그래디언트가 계산될 수 있도록 만들어, fp32에서 fp16으로 정밀도를 낮추었을 때 계산된 대부분의 그래디언트가 0으로 처리되는 것을 방지한다\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    222\u001b[39m \u001b[33;03m2. https://docs.pytorch.org/docs/stable/amp.html\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.scale(loss).backward()\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.update()\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# target network 갱신\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\gradenv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\gradenv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\gradenv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from lib.env_generator2 import PathFindEnv\n",
    "\n",
    "env = PathFindEnv(height=32, width=32)\n",
    "dqn = DQN(train=True)\n",
    "\n",
    "result = dqn.train(env, max_env_creation_cnt=10, max_ep_win_in_env=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env 10/10: Episode 860 Done!, Current eps_threshold: 0.24042293704298695, Total win episode/Goal win episode = (15/15)\n",
      "===== Train Done =====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50],\n",
       " [-30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  ...],\n",
       " [-30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  50,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  50],\n",
       " [-30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50],\n",
       " [-30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50],\n",
       " [-30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50],\n",
       " [-30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  ...],\n",
       " [-30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  ...],\n",
       " [-30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  50,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  -30,\n",
       "  50]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.env_generator2 import PathFindEnv\n",
    "\n",
    "env = PathFindEnv(height=32, width=32)\n",
    "dqn = DQN(train=True)\n",
    "\n",
    "checkpoint = os.path.join(os.getcwd(), \"checkpoints\", \"checkpoint_1_10_15_2025-08-09.pth\")\n",
    "dqn.plot_mv_avgs(dqn.train(env, checkpoint=checkpoint, max_env_creation_cnt=10, max_ep_win_in_env=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = PathFindEnv(height=32, width=32)\n",
    "dqn = DQN(train=True)\n",
    "init_state = env.reset(fixed=False)\n",
    "\n",
    "checkpoint = os.path.join(os.getcwd(), \"checkpoints\", \"checkpoint_10_10_15_2025-08-10.pth\")\n",
    "dqn.inference(env, init_state, checkpoint)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM+S1b0iMHT0uW6eQ6lGz39",
   "gpuType": "T4",
   "mount_file_id": "1olYyr74UOwDeBvK63DEFWDI11766JpOc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
